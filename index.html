<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>AI-Powered Document Reader for Low-Vision Users</title>
    <link rel="stylesheet" href="styles.css" />
  </head>
  <body>
    <div class="container">
      <h1 id="top">AI-Powered Document Reader for Low-Vision Users</h1>
      <h2>Team Members</h2>
      <ul>
        <li>
          Jahazel Sanchez, Verrels Lukman Eugeneo, Asya Lyubavina, Jerry Onyango
        </li>
      </ul>
      <h2>Abstract</h2>
      <p>
        The goal of this project is to develop an accessible AI-powered web
        application that enables low-vision users to independently interpret
        complex documents containing text and images. Globally, an estimated 2.2
        billion people have vision impairments, with many facing significant
        barriers when accessing digital content due to inadequate support for
        interpreting visual and layout information. While numerous
        text-to-speech tools exist, few address these specific needs,
        particularly regarding descriptive context for visual elements. Our
        application directly resolves this critical accessibility gap by
        integrating robust Optical Character Recognition (OCR) and advanced
        transformer-based image captioning models to deliver comprehensive,
        contextual descriptions, significantly enhancing independent interaction
        for low-vision users.
      </p>
      <h2>Introduction</h2>
      <p>
        Visual information is integral to everyday life, offering essential
        context that textual descriptions alone cannot fully capture. For
        low-vision individuals, independently interpreting this visual data
        often poses substantial challenges. Many existing accessibility
        solutions, while helpful, fall short by neglecting critical visual
        components such as images or detailed layout structures. These omissions
        significantly limit practicality and usability, leaving users with
        incomplete understandings of digital documents.
      </p>
      <p>
        Our project specifically targets these shortcomings through a targeted,
        AI-driven web application designed explicitly for comprehensive document
        interpretation. By utilizing Tesseract OCR
        <a href="#ref1">[1]</a> selected for its demonstrated accuracy and
        reliability across varied document types, we ensure robust text
        extraction. Furthermore, we incorporate HuggingFace transformer models
        <a href="#ref2">[2]</a> such as BLIP and VisionEncoderDecoder, chosen
        for their superior multimodal capabilities and proven ability to produce
        contextually relevant image captions. Unlike generic captioning
        approaches, our chosen models explicitly consider the contextual
        interplay between textual and visual elements, ensuring users receive
        meaningful, integrated interpretations. To further enhance usability,
        our solution employs web frameworks such as Gradio
        <a href="#ref3">[3]</a> or Streamlit <a href="#ref4">[4]</a> , known for
        their accessible, interactive, and inclusive interface designs
        particularly suitable for visually impaired audiences.
      </p>

      <p>
        Explicitly addressing the technical challenges inherent to this project
        reinforces the practicality of our solution. These challenges include
        accurately segmenting complex and diverse document layouts and
        interpreting a broad spectrum of imagery, ranging from simple graphics
        to intricate diagrams and visual tables. Transformer-based models, while
        powerful, often encounter difficulty with context-dependent or
        domain-specific imagery, occasionally producing ambiguous or inaccurate
        outputs. Additionally, the critical task of creating an intuitive and
        accessible user interface for low-vision users requires rigorous,
        iterative feedback-driven design and testing, particularly as our team
        does not personally experience these impairments.
      </p>

      <p>
        By proactively integrating user-centered design principles and
        continually involving user feedback, our approach directly addresses
        these challenges, ensuring that the final product accurately meets
        real-world accessibility needs.
      </p>

      <h2>Related Work</h2>
      <p>
        Several previous efforts have informed and shaped our project, yet each
        contains gaps that our solution specifically targets and resolves. For
        example, Smith's ImageAssist (2021) <a href="#ref5">[5]</a> innovatively
        enabled visually impaired users to interactively explore images but
        focused primarily on isolated, exploratory interactions rather than
        integrated interpretation of mixed textual-visual content within
        documents. Our application explicitly advances beyond this by providing
        cohesive, contextually integrated descriptions for entire documents,
        thus filling a crucial accessibility gap left by ImageAssist.
      </p>

      <p>
        Similarly, Vijayanarayanan et al. (2023)
        <a href="#ref6">[6]</a> successfully combined OCR with text-to-speech
        for textual accessibility, effectively demonstrating OCR's strengths.
        However, their solution overlooks the critical task of interpreting
        visual elements and complex document layouts. By specifically
        integrating sophisticated HuggingFace transformer-based image captioning
        models, our approach directly resolves this limitation, offering
        comprehensive visual-textual integration essential for complete document
        understanding.
      </p>
      <p>
        Similarly, Vijayanarayanan et al. (2023)<a href="#ref6">[6]</a>
        demonstrated the effectiveness of combining Optical Character
        Recognition (OCR) and text-to-speech to support visually impaired users
        in efficiently processing textual content. While their approach
        highlights the strengths of OCR technology, it lacks the capability to
        interpret images or complex document layouts. Our project addresses this
        critical gap by integrating advanced HuggingFace transformer models
        specifically optimized for context-rich image captioning, ensuring
        visual elements are fully integrated into the documentâ€™s accessible
        description.
      </p>
      <p>
        Moreover, Bodi et al. (2021) <a href="#ref7">[7]</a> demonstrated
        powerful AI-driven contextual descriptions within dynamic visual
        environments (videos), developing tools such as NarrationBot and
        InfoBot. Despite their significance, these tools remain specialized for
        dynamic contexts, leaving the accessibility needs of static, image-rich
        documents largely unaddressed. Our solution specifically adapts
        AI-driven narrative generation techniques to static documents, directly
        extending AI capabilities into this underserved yet essential domain.
      </p>
      <p>
        Collectively, our project synthesizes these foundational insights while
        explicitly resolving their individual limitations, delivering a uniquely
        comprehensive, robust accessibility solution tailored precisely to the
        critical needs of low-vision users interpreting complex visual-textual
        documents.
      </p>
      <h2>References</h2>
      <p id="ref1">
        [1] Tesseract OCR.
        <a href="https://github.com/tesseract-ocr/tesseract"
          >https://github.com/tesseract-ocr/tesseract</a
        >
        <a href="#top">(Back to text)</a>
      </p>
      <p id="ref2">
        [2] HuggingFace Transformers.
        <a href="https://huggingface.co/transformers/"
          >https://huggingface.co/transformers/</a
        >
        <a href="#top">(Back to text)</a>
      </p>
      <p id="ref3">
        [3] Gradio.
        <a href="https://gradio.app/">https://gradio.app/</a>
        <a href="#top">(Back to text)</a>
      </p>
      <p id="ref4">
        [4] Streamlit.
        <a href="https://streamlit.io/">https://streamlit.io/</a>
        <a href="#top">(Back to text)</a>
      </p>
      <p id="ref5">
        [5] Smith, B. A. (2021). "ImageAssist: Enhancing Image Understanding for
        Low Vision Users." Proceedings of CHI.
        <a href="#top">(Back to text)</a>
      </p>
      <p id="ref6">
        [6] Vijayanarayanan et al. (2023). "Image Processing Based on Optical
        Character Recognition with Text-to-Speech for Visually Impaired."
        <em>Journal of Scientific and Engineering Research, 6(4).</em>
        <a href="#top">(Back to text)</a>
      </p>
      <p id="ref7">
        [7] Bodi et al. (2021). "Automated Video Description for Blind and Low
        Vision Users."
        <em
          >Proceedings of CHI Conference on Human Factors in Computing
          Systems.</em
        >
        <a href="#top">(Back to text)</a>
      </p>
    </div>
  </body>
</html>
